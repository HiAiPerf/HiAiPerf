# ARCHITECTURE.md: AI Public Speaking Coach

This document outlines the architecture of the AI Public Speaking Coach application, which provides users with textual and audio feedback on their public speaking videos. The application leverages Google Cloud services for advanced AI capabilities like Speech-to-Text, Generative AI (Gemini), and Text-to-Speech, orchestrated by a LangGraph agent, and presented through a Gradio-based user interface.

## 1. High-Level Architecture

The application follows a client-server pattern, where the Gradio application acts as both the user interface and a lightweight server orchestrating calls to various Google Cloud APIs.

![Architecture Diagram](images/architecture-final.png)


## 2. Components

The application is composed of several Python modules, each with a distinct responsibility:

### 2.1. `app_gradio.py` (User Interface Layer)

* **Purpose:** Serves as the main entry point for the user interface. It defines the Gradio UI components (video input, text output, audio output, buttons) and orchestrates the user interaction.
* **Key Responsibilities:**
    * Initializes the Gradio `Blocks` interface with custom CSS for a mobile-friendly aesthetic.
    * Handles video file uploads/recordings from the user.
    * Calls the core AI processing logic (`get_speech_feedback` function), which initiates the LangGraph agent.
    * Receives and displays textual feedback.
    * Receives and plays synthesized audio feedback.
    * Manages UI state (loading indicators, button interactivity).
    * Cleans up local temporary files generated by Gradio.

### 2.2. `agent_graph.py` (Orchestration Layer - LangGraph Agent)

* **Purpose:** Defines the sequential workflow of the AI coaching process using the LangGraph library. This acts as the **Planner** and **Executor** of the agent.
* **Key Responsibilities:**
    * **Planner:** Defines the sequence of steps (nodes) required to process a user request, from video input to feedback generation.
    * **Executor:** Manages the execution flow, ensuring each node is run in the correct order and handles state transitions between them.
    * Constructs a directed graph representing the steps of the public speaking analysis.
    * Defines the order and dependencies of the nodes (functions) in the workflow.
    * Manages the state transitions between different processing steps.

### 2.3. `agent_nodes.py` (Business Logic / Tool Integration Layer)

* **Purpose:** Contains the individual "nodes" (functions) that perform specific tasks within the LangGraph workflow, integrating with various Google Cloud AI services (acting as **Tools**).
* **Key Responsibilities:**
    * **`PublicSpeakingState` (Memory Structure):** A `TypedDict` that defines the shared memory structure for the LangGraph agent. It holds the state of the coaching session as it progresses through the graph, including:
        * `video_gcs_uri`: GCS URI of the original uploaded video.
        * `extracted_audio_gcs_uri`: GCS URI of the audio extracted from video.
        * `transcript`: Text transcript of the speech.
        * `feedback_text`: AI-generated textual feedback.
        * `feedback_audio_gcs_uri`: GCS URI of the synthesized audio feedback.
    * **`node_extract_audio`:** Extracts audio from the video stored in GCS.
        * **Tool Integration:** Uses `ffmpeg-python` for local audio processing.
    * **`node_transcribe_audio`:** Transcribes the extracted audio into text.
        * **Tool Integration:** Calls **Google Cloud Speech-to-Text API**.
    * **`node_coach_feedback`:** Generates public speaking feedback based on the transcript.
        * **Tool Integration:** Calls **Google Gemini** (via LangChain's `ChatGoogleGenerativeAI`).
    * **`node_synthesize_audio_feedback`:** Converts the textual feedback into natural-sounding audio.
        * **Tool Integration:** Calls **Google Cloud Text-to-Speech API**.
    * **`node_cleanup` (Implicit in `finally`):** While not a separate node, the `finally` block in `app_gradio.py` handles cleanup of GCS blobs and local files, which is a crucial part of the process.

### 2.4. `utils.py` (Utility Functions)

* **Purpose:** Provides common helper functions used across different modules, primarily for Google Cloud Storage (GCS) interactions and multimedia processing.
* **Key Responsibilities:**
    * `upload_to_gcs`: Uploads local files to a specified GCS bucket.
    * `download_from_gcs`: Downloads files from GCS to a local path.
    * `delete_gcs_blob`: Deletes objects from GCS.
    * `extract_audio_from_video`: Extracts and resamples audio from a video file using `ffmpeg-python`.
    * `GCS_BUCKET_NAME`: Defines the Google Cloud Storage bucket used for temporary file storage.

## 3. Data Flow

1.  **User Request:** The user uploads or records a video via the Gradio UI (`app_gradio.py`).
2.  **Initial State & Orchestration:** `app_gradio.py` uploads the video to GCS and initiates the LangGraph agent (`agent_graph.py`) with the video's GCS URI as part of the `PublicSpeakingState`. The LangGraph agent acts as the central **Executor**, driving the process.
3.  **Node Execution (Tools in Action):**
    * `node_extract_audio` downloads the video from GCS, extracts audio locally using FFmpeg, and re-uploads the extracted audio to GCS. The `PublicSpeakingState` is updated with the audio's GCS URI.
    * `node_transcribe_audio` uses the GCS audio URI to call the Google Cloud Speech-to-Text API, obtaining a text transcript. The `PublicSpeakingState` is updated with the transcript.
    * `node_coach_feedback` takes the transcript from the `PublicSpeakingState` and sends it to the Google Gemini model (via LangChain) to generate textual public speaking feedback. The `PublicSpeakingState` is updated with the feedback text.
    * `node_synthesize_audio_feedback` takes the textual feedback from `PublicSpeakingState`, cleans it, sends it to the Google Cloud Text-to-Speech API to synthesize audio feedback, and uploads this audio to GCS. The `PublicSpeakingState` is updated with the audio's GCS URI.
4.  **Feedback Delivery:** The `app_gradio.py` function downloads the synthesized audio feedback from GCS to a local temporary path and returns both the textual and local audio path to the Gradio UI.
5.  **UI Display:** The Gradio UI displays the textual feedback and plays the audio feedback.
6.  **Cleanup:** Temporary files in GCS and locally are cleaned up after the process completes, ensuring efficient resource management.

## 4. Authentication & Authorization

The application uses Google Cloud's authentication mechanisms:

* **Local Development (Docker Container):**
    * **Service Account Key:** Credentials are provided to the Docker container by mounting a service account JSON key file (`sa-key.json`) and setting the `GOOGLE_APPLICATION_CREDENTIALS` environment variable.
    * The service account requires specific IAM roles: `Storage Object Admin`, `Speech-to-Text User`, `Vertex AI User`, and `Editor` (as a broad role to cover Text-to-Speech permissions).
* **Deployment (e.g., Google Cloud Run):**
    * In a production deployment, the application would typically run under a **service account** attached to the compute resource (e.g., Cloud Run service). This service account would need the same IAM roles granted.

## 5. Logging and Observability

The application incorporates basic logging to provide insights into its operation and aid in debugging:

* **Console Output:** Key events, such as file uploads/downloads, audio extraction, transcription status, and feedback generation, are printed to the console (`stdout`). When running in Docker, these logs are accessible via `docker logs <container_id>`.
* **Error Handling:** `try-except` blocks are used to catch and print errors, including full tracebacks, to the console. This helps in identifying issues within specific nodes or API calls.
* **Gradio UI Warnings/Errors:** Gradio's `gr.Warning` and `gr.Error` are used to display user-friendly messages directly in the UI for immediate feedback on common issues (e.g., no video uploaded, processing errors).
* **Google Cloud Logging (Implicit):** When deployed on Google Cloud services (like Cloud Run), the console output (`stdout`/`stderr`) will automatically be ingested by Google Cloud Logging, providing centralized log management, filtering, and analysis capabilities.

## 6. External Dependencies

### 6.1. Google Cloud Services (Tools)

* **Google Cloud Storage (GCS):** For temporary storage of video, extracted audio, and synthesized audio.
* **Google Cloud Speech-to-Text API:** For transcribing spoken content from video audio.
* **Google Gemini (via Vertex AI Generative AI API):** For generating intelligent public speaking feedback.
* **Google Cloud Text-to-Speech API:** For synthesizing natural-sounding audio from the generated text feedback.

### 6.2. Python Libraries

* `gradio`: For building the interactive web user interface.
* `langgraph`: For orchestrating the multi-step AI workflow (Planner/Executor).
* `langchain-google-genai`: LangChain integration for Google Gemini models.
* `google-cloud-storage`: Python client library for GCS.
* `google-cloud-speech`: Python client library for Speech-to-Text.
* `google-cloud-texttospeech`: Python client library for Text-to-Speech.
* `pydub`: For audio manipulation (e.g., resampling) during audio extraction.
* `ffmpeg-python`: Pythonic wrapper for FFmpeg, used for video/audio processing.
* `uuid`: For generating unique identifiers for temporary files and sessions.
* `tempfile`: For managing temporary files in the local filesystem.
* `re`: For regular expression operations (e.g., stripping markdown).

This architecture provides a robust and modular design for your AI Public Speaking Coach, separating concerns and leveraging powerful cloud AI services.
