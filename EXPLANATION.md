# EXPLANATION.md: Agent Details for AI Public Speaking Coach

This document provides a deeper dive into the internal workings of the AI Public Speaking Coach, focusing on the LangGraph agent's reasoning, memory, planning, tool usage, and current limitations.

## 1. Agent's Reasoning Process

The core reasoning in this Public Speaking AI Coach is primarily performed by the **Google Gemini model** within the `node_coach_feedback` function. The LangGraph agent itself acts more as an **orchestrator and executor** of a predefined workflow rather than a dynamic, self-reasoning agent that plans its own steps on the fly.

* **Input to Reasoning:** The Gemini model receives the full transcript of the user's speech, extracted and transcribed by earlier nodes in the graph.

* **Prompt-Based Reasoning:** The reasoning is guided by a carefully crafted prompt that instructs Gemini to act as an "expert public speaking coach." This prompt explicitly asks Gemini to:

    * Identify **strengths** (2-3 specific positive aspects).

    * Identify **areas for improvement** (2-3 specific, actionable suggestions).

    * Provide **overall encouragement**.

    * Maintain a **natural, conversational, and supportive tone**.

* **Output of Reasoning:** Gemini's output is the textual feedback, which then proceeds to the Text-to-Speech synthesis.

The agent's "reasoning" at the LangGraph level is limited to following the pre-defined sequence of operations based on the successful completion of the previous step. It doesn't dynamically choose which tool to use next or adapt its plan based on intermediate reasoning outcomes beyond basic success/failure.

## 2. Memory Usage

The agent's memory is managed through the `PublicSpeakingState` `TypedDict`, which is passed between the nodes in the LangGraph. This state object serves as the central repository for all relevant information generated and consumed throughout a single coaching session.

The `PublicSpeakingState` holds the following key pieces of information:

* `video_gcs_uri`: Stores the Google Cloud Storage URI of the original video uploaded by the user. This is the initial input to the agent's memory.

* `extracted_audio_gcs_uri`: Stores the GCS URI of the audio extracted from the video. This is added to memory by `node_extract_audio`.

* `transcript`: Stores the full text transcription of the speech. This is added to memory by `node_transcribe_audio`.

* `feedback_text`: Stores the AI-generated textual feedback from Gemini. This is added to memory by `node_coach_feedback`.

* `feedback_audio_gcs_uri`: Stores the GCS URI of the synthesized audio feedback. This is added to memory by `node_synthesize_audio_feedback`.

This memory structure ensures that each subsequent node has access to the necessary data generated by preceding nodes, allowing for a sequential and dependent workflow. The memory is ephemeral for each session; it's not persistently stored in a database beyond the duration of the request.

## 3. Planning Style

The agent employs a **fixed, sequential planning style**. This means the order of operations is hardcoded within the `agent_graph.py` file as a linear chain of nodes. There is no dynamic planning, conditional branching, or self-correction of the plan during execution.

The planning sequence is as follows:

1.  **Extract Audio:** From the input video.

2.  **Transcribe Audio:** Convert audio to text.

3.  **Coach Feedback:** Generate text feedback from the transcript.

4.  **Synthesize Audio Feedback:** Convert text feedback to audio.

5.  **(Implicit) Cleanup:** Delete temporary files.

This style is suitable for well-defined, predictable workflows where the steps are always the same regardless of the input.

## 4. Tool Integration

The agent integrates with various external "tools" (Google Cloud APIs and local utilities) through its `agent_nodes.py` functions. Each node acts as a wrapper around a specific tool, handling its input, execution, and output.

The primary tools integrated are:

* **FFmpeg (via `ffmpeg-python`):**

    * **Purpose:** Used by `node_extract_audio` for robust audio extraction and resampling from various video formats.

    * **Integration:** `ffmpeg-python` provides a Pythonic interface to the FFmpeg command-line tool, allowing for precise control over multimedia processing.

* **Google Cloud Speech-to-Text API:**

    * **Purpose:** Used by `node_transcribe_audio` to convert spoken language in audio files into written text.

    * **Integration:** Utilizes the `google-cloud-speech` Python client library to send audio data (via GCS URI) for long-running transcription and retrieve the results.

* **Google Gemini (via `langchain_google_genai`):**

    * **Purpose:** Used by `node_coach_feedback` to generate intelligent and nuanced public speaking feedback.

    * **Integration:** Leverages LangChain's `ChatGoogleGenerativeAI` wrapper, which simplifies interaction with the Gemini model, allowing for